# Incertidumbre en las predicciones

```{r, include = FALSE}
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 13))
knitr::opts_chunk$set(fig.width=4, fig.height=3) 
cbb_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
scale_colour_discrete <- function(...) {
  ggplot2::scale_colour_manual(..., values = cbb_palette)
}
```

En todos los casos hasta ahora, vimos cómo hacer: 

- Predicciones puntuales (un solo valor) 
- Evaluar un conjunto razonablemente grande de predicciones 
con el error de predicción, que es un promedio de la pérdida seleccionada.

Cuando vamos a hacer muchas predicciones, y la pérdida se alínea con 
los costos reales de la decisión, entonces minimizar la pérdida promedio (por ejemplo
cuadrática, absoluta o relativa) 
puede ser un criterio suficiente para decidir cómo utilizar las predicciones en 
nuestra decisión. Esto es especialmente cierto si la pérdida promedio es chica 
y no detectamos errores atípicos que podrían ser muy costosos. 

En el ejemplo de la primera sección de lectura de medidores, el
resultado tuvo precisión relativamente alta, y quizá unos cuantos errores grandes en algunas
lecturas pueden corregirse posteriormente (reclamos, por ejemplo) sin un costo 
adicional muy grande. Otro ejemplo es obtener las coordenadas del rectángulo que
acota un objeto de interés en una imagen para crear álbumes de fotografía automatizados.
En este caso, podríamos ver si el uso de las predicciones puntuales son suficientes
para que esos álbumes sean una característica deseable para los usuarios.

Sin embargo, en muchos problemas reales:

- La decisión que se toma tiene costos asociados más complicados que en un principio
pueden ser difíciles de cuantificar.
- Estamos tomando un número relativamente chico de decisiones, y existe el riesgo
de predicciones considerablemente malas.

En estos casos, quisiéramos expresar con más precisión y de manera útil la incertidumbre que tenemos
acerca de predicciones particulares, y el error promedio puede ser engañoso. Por ejemplo:


- Queremos construir un portafolio de bienes raíces usando un modelo predictivo de
sus precios. En este caso, el riesgo de cada compra debe expresarse de manera clara.
Compras con mucha incertidumbre pueden ser apropiadas para algunos compradores, pero
poco apropiadas para otros. En estos casos, requerimos expresar la incertidumbre asociada
con cada compra.
- Queremos predecir los resultados de una elección a partir de encuestas, datos macroeconómicos,
etc. Usualmente estos resultados tienen incertidumbre considerable, y para evaluar y utilizar
correctamente esos pronósticos es necesario saber su incertidumbre.

En estos casos, puede ser útil construir, en lugar de predicciones puntuales, **intervalos
de predicción**. Hay muchos enfoques para construir estos intervalos: puedes estudiarlo
en un curso estándar de regresión, regresión bayesiana (ve por ejemplo
@GelmanHill), y también 
existen métodos computacionales basados en simulación de aplicación más general.
Siempre es mejor tener intervalos
*construidos apropiadamente* que no tenerlos, pero tenemos que considerar que esto
requiere más recursos y riesgos, en particular:

- (Recursos) Costo computacional adicional considerable y/o
- (Riesgos) Recaer en supuestos teóricos (muchas veces asintóticos) y diagnósticos de modelos, y
- (Recursos) Validación adicional de modelos

En caso de producir intervalos, es importante de cualquier forma checar directamente
que nuestros rangos de incertidumbre son apropiados (para lo cual también es útil
muestras de prueba independientes del entrenamiento). Sin muestras de prueba, 
puede ser muy difícil evaluar qué tan buenas son nuestras estimaciones de la incertidumbre.
Casi todos los modelos tienen fallas en este sentido.

Por otro lado, en algunas aplicaciones tener un error *promedio* razonable es suficiente,
necesitamos tomar un número relativamente grande de decisiones,
con costos relativamente uniformes (desde el punto de vista del negocio o de daños posibles)
los estimadores puntuales pueden ser suficientes cuando el error de predicción
es suficientemente bajo.  Siempre revisa los errores individuales con la muestra prueba, y
considera si es necesario calcular y checar intervalos predictivos.


## Intervalos con muestra de prueba

Veremos la el método *split-conformal* para construir intervalos para predicciones.
Aprovechamos naturalmente nuestra división de la muestra para producir
intervalos la primer garantía básica de cobertura, que por el momento supondremos
que queremos que sea $90%$:

```{block2, type = "comentarios"}
**Intervalos predictivos con muestra de prueba**

1. Construirmos nuestro predictor $\hat{f}(x)$ con la muestra de entrenamiento
2. Consideramos los residuales $\mathbf{y}_i - \hat{f}(\mathbf{x}_i)$ en la muestra
de prueba, y calculamos su cuantiles $q_{0.05}$ y $q_{0.95}$.  
3. Supongamos que $(\mathbf{x}, \mathbf{y})$ es una nueva observación. Su intervalo
predictivo es $I(\mathbf{x}) = [\hat{f}(\mathbf{x}) - q_{0.05}, \hat{f}(\mathbf{x}) + q_{0.95}]$
```


Entonces, solamente requiriendo muestras independientes tomadas de la misma
población, sin ningún otro supuesto
acerca del predictor o el ajuste (ver [Distribution-Free Predictive Inference for Regression](https://www.stat.cmu.edu/~ryantibs/papers/conformal.pdf)), tenemos que 
$$P(\mathbf{y} \in I(\mathbf{x}))\geq 0.90.$$ 
Adicionalmente, si la muestra de prueba no es muy chica, la cobertura real es cercana al 90\%.

Esta cobertura es confiable y robusta, sin embargo, se requieren algunos elementos adicionales
para que los intervalos sean más útiles. Quisiéramos también que la cobertura
local alrededor de cualquier región del las $x$'s sea cercana al 90% (cobertura
condicional). Esto es más
dificíl de alcanzar, y en general, solo se satisface si se complen ciertos supuestos
que en general es difícil verificar si se cumplen o no realmente, como la linealidad
en problemas de dimensión alta.

Un chequeo básico que podemos hacer es el siguiente:

- Para cada nivel de predicción $\hat{y} = \hat{f}(x)$, los intervalos tienen cobertura de 90%.

Como esto sólo involucra las predicciones, los valores observados, y los intervalos predictivos,
esto puede checarse en la práctica con una tabla o gráfica. En caso de que
no se cumpla esta condición, podemos usar una muestra independiente adicional 
de **calibración**, y mejorar esta cobertura condicional.


## Ejemplo: bodyfat

En el ejemplo de grasa corporal utilizamos lasso para seleccionar un modelo. Podemos
construir intervalos predictivos (*split-conformal*) como sigue:


```{r, message = FALSE}
library(tidyverse)
library(tidymodels)
library(gt)
dat_grasa <- read_csv(file = '../datos/bodyfat.csv') 
set.seed(183)
grasa_particion <- initial_split(dat_grasa, 0.5)
grasa_ent <- training(grasa_particion)
grasa_pr <- testing(grasa_particion)
grasa_receta <- recipe(grasacorp ~ ., grasa_ent) 
# lasso
modelo_gc <- linear_reg(mixture = 1, penalty = 1.5) %>% 
  set_engine("glmnet", lambda.min.ratio = 1e-20) 
flujo_2 <- workflow() %>% 
  add_model(modelo_gc) %>% 
  add_recipe(grasa_receta)
# ajuste
flujo_2 <- flujo_2 %>% fit(grasa_ent)
```

Ahora, sobre la muestra de prueba evaluamos el error y calculamos residuales
y su distribución:

```{r}
preds_gc_tbl <- predict(flujo_2, grasa_pr) |> 
  bind_cols(grasa_pr) |> 
  mutate(residual = grasacorp - .pred)
ggplot(preds_gc_tbl, aes(x = residual)) +
  geom_histogram(binwidth = 2)
intervalo_r <- quantile(preds_gc_tbl |> pull(residual), probs = c(0.05, 0.95))
intervalo_r |> round(2)
```
Y finalmente, podemos hacer distintas verificaciones para ver cómo se comportan los residuales, por 
ejemplo:

```{r}
preds_gc_tbl <- preds_gc_tbl |> 
  mutate(inf = .pred + intervalo_r[1], sup = .pred + intervalo_r[2])
ggplot(preds_gc_tbl, aes(x = abdomen, ymin = inf, ymax = sup, y = grasacorp)) +
  geom_point(colour = "red") +
  geom_linerange()
```

Una gráfica útil es la de predicciones en la escala de las $x$ 
contra intervalos y observados en el eje $y$.

```{r}
ggplot(preds_gc_tbl, aes(x = .pred, ymin = inf, ymax = sup, y = grasacorp)) +
  geom_abline() +
  geom_point(colour = "red") +
  geom_linerange(alpha = 0.5) 
```
Donde vemos que la cobertura parece ser similar para todos los rangos de valores
de predicción. Podemos hacer también una tabla simple como la que sigue para verificar
la cobertura:

```{r}
n_grupos <- 4 # podemos usar más grupos si tenemos más datos
preds_gc_tbl |> 
  rename(y = grasacorp) |> 
  mutate(grupo_pred = cut_number(.pred, n = n_grupos)) |> 
  group_by(grupo_pred) |> 
  summarise(n = n(), cobertura = mean(y >= inf & y <= sup)) |>
  mutate(error_e = 2 * sqrt(cobertura * (1- cobertura) / n)) |> 
  gt() |> fmt_number(where(is_double), decimals = 2)
```


## Ejemplo: rendimiento

Revisamos nuestro ejemplo de rendimiento de coches:

```{r, message = FALSE}
library(tidyverse)
library(tidymodels)
library(gt)
auto <- read_csv("../datos/auto.csv")
datos <- auto[, c('name', 'weight','year', 'mpg', 'displacement')]
datos <- datos %>% mutate(
  peso_kg = weight * 0.45359237,
  rendimiento_kpl = mpg * (1.609344 / 3.78541178), 
  año = year)
```

Vamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos
hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así
obtenemos alrededor de 100 casos para prueba):

```{r, message = FALSE}
set.seed(121)
datos_split <- initial_split(datos, prop = 0.3)
datos_entrena <- training(datos_split)
datos_prueba <- testing(datos_split)
# preprocesamiento y flujo
receta_lineal <- recipe(rendimiento_kpl ~ peso_kg + año, datos_entrena) |> 
  step_ns(peso_kg, deg_free = 3) |> 
  step_ns(año, deg_free = 2)
mod_lineal <- linear_reg() |>  
  set_engine("lm")  
flujo <- workflow() |>  
  add_recipe(receta_lineal) |> 
  add_model(mod_lineal)
flujo_ajustado <- fit(flujo, datos_entrena)
```

Una vez que tenemos el flujo ajustado, podemos ver cómo se comportan los residuales:

```{r}
preds_tbl <- predict(flujo_ajustado, datos_prueba) |> 
  bind_cols(datos_prueba) |> 
  mutate(e = rendimiento_kpl - .pred)
ggplot(preds_tbl, aes(x = e)) + geom_histogram()
```
Podemos pensar que estos son Y podemos calcular cuantiles:

```{r}
cuantiles_e <- quantile(preds_tbl |> pull(e), probs = c(0.05, 0.95)) |> 
  round(2)
cuantiles_e
```

Podemos ahora construir intervalos de 90% para nuestras predicciones de la siguiente
forma:

```{r}
preds_tbl <- preds_tbl |> 
  mutate(inf = .pred + cuantiles_e["5%"], 
         sup = .pred + cuantiles_e["95%"])
```
Por definición, estos intervalos tienen cobertura promedio sobre un conjunto 
de prueba de aproximadamente el 90%. 

Sin embargo, la cobertura puede nos ser uniforme en distintas regiones de las
entradas. Checamos primero la cobertura dependiendo de la predicción:

```{r}
ggplot(preds_tbl, aes(x = peso_kg, y =  rendimiento_kpl, ymin = inf, ymax = sup)) +
  geom_linerange() +
  geom_point(colour = "red") 
```

Y vemos que aunque nuestros intervalos son del 90%, tienen baja cobertura para
pesos bajos y sobrecubren para pesos altos. Estos intervalos son poco útiles
por esa razón: exageran la incertidumbre para valores altos de peso y 
la subestiman para valores bajos de peso. Notamos lo mismo comparando los
valores de $y$ dada la predicción:

```{r}
ggplot(preds_tbl, aes(x = .pred, y = rendimiento_kpl, ymin = inf, ymax = sup)) +
  geom_linerange() +
  geom_point(colour = "red") + coord_obs_pred()
```

También podemos verificar con una tabla que la cobertura no es uniforme:

```{r}
n_grupos <- 6 # podemos usar más grupos si tenemos más datos
preds_tbl |> 
  rename(y = rendimiento_kpl) |> 
  mutate(grupo_pred = cut_number(.pred, n = n_grupos)) |> 
  group_by(grupo_pred) |> 
  summarise(n = n(), cobertura = mean(y >= inf & y <= sup)) |>
  mutate(error_e = 2 * sqrt(cobertura * (1- cobertura) / n)) |> 
  gt() |> fmt_number(where(is_double), decimals = 2)
```


Podemos mejorar los intervalos usando regresión cuantílica. Primero dividimos la
muestra de prueba en una muestra de calibración y otra de evaluación:

```{r}
lambda <- 1
calib_eval_split <- initial_split(preds_tbl, 0.5)
calibracion_tbl <- training(calib_eval_split)
eval_tbl <- testing(calib_eval_split)
ggplot(calibracion_tbl, aes(x = .pred, y = rendimiento_kpl)) +  
  geom_point() +
  geom_quantile(method = "rqss", lambda = lambda, quantiles = c(0.05, 0.95))
sup_qreg <- quantreg::rqss(rendimiento_kpl ~ .pred, lambda = lambda, data = preds_tbl, tau = 0.95)
inf_qreg <- quantreg::rqss(rendimiento_kpl ~ .pred, lambda = lambda, data = preds_tbl, tau = 0.05)
calibracion_tbl <- calibracion_tbl |> 
  mutate(sup_q = predict(sup_qreg, newdata = calibracion_tbl),
         inf_q = predict(inf_qreg, newdata = calibracion_tbl))
```

Los nuevos intervalos se ven como sigue:

```{r}
ggplot(calibracion_tbl, aes(x = peso_kg, y =  rendimiento_kpl, ymin = inf_q, ymax = sup_q)) +
  geom_linerange() +
  geom_point(colour = "red") 
```

Finalmente verificamos con la muestra de evaluación (en este punto, podemos ajustar la regresión
cuantílica si es necesario):

```{r}
eval_tbl <- eval_tbl |> 
  mutate(sup_q = predict(sup_qreg, newdata = eval_tbl),
         inf_q = predict(inf_qreg, newdata = eval_tbl))
```

```{r}
n_grupos <- 5 # podemos usar más grupos si tenemos más datos
eval_tbl |> 
  rename(y = rendimiento_kpl) |> 
  mutate(grupo_pred = cut_number(.pred, n = n_grupos)) |> 
  group_by(grupo_pred) |> 
  summarise(n = n(), cobertura = mean(y >= inf_q & y <= sup_q), .pred = mean(.pred)) |>
  mutate(error_e = 2 * sqrt(cobertura * (1- cobertura) / n)) |> 
  ggplot(aes(x = .pred, y = cobertura, ymin = cobertura - error_e, ymax = cobertura + error_e)) +
  geom_hline(yintercept = 0.9, colour = "red") +
  geom_point() + geom_linerange() +
  ylim(c(0,1.1))
```

En este caso, podemos construir nuevos intervalos utilizando los datos de evaluación.

## Ejemplo: cambios en el modelo

En algunos casos podemos seleccionar una variable respuesta diferente o la función
de pérdida para obtener mejores propiedades de nuestros intervalos. En este
ejemplo, escogemos como variable respuesta el precio por metro cuadrado sobre
el área habitable.

```{r}
source("../R/casas_traducir_geo.R")
set.seed(83)
casas_split <- initial_split(casas, prop = 0.75)
casas_entrena <- training(casas_split)
receta_casas <- recipe(precio_m2_miles ~ 
           nombre_zona + 
           area_hab_m2 + area_garage_m2 + area_sotano_m2 + 
           area_2o_piso_m2 +
           area_lote_m2 + 
           año_construccion + 
           calidad_gral + calidad_garage + calidad_sotano +
           condicion_gral + 
           num_coches  + 
           aire_acondicionado + condicion_venta, 
           data = casas_entrena) |> 
  step_filter(condicion_venta == "Normal") |> 
  step_select(-condicion_venta, skip = TRUE) |> 
  step_mutate(tiene_2o_piso = ifelse(area_2o_piso_m2 == 0, 1, 0)) |> 
  step_mutate(area_sotano_m2 = ifelse(is.na(area_sotano_m2), 0, area_sotano_m2)) |> 
  step_mutate(area_garage_m2 = ifelse(is.na(area_garage_m2), 0, area_garage_m2)) |> 
  step_ratio(starts_with("area"), denom = denom_vars(area_hab_m2)) |> 
  step_novel(nombre_zona, calidad_sotano, calidad_garage) |> 
  step_ns(calidad_gral, deg_free = 2) |> 
  step_ns(condicion_gral, deg_free = 2) |> 
  step_ns(starts_with("area_lote"), deg_free = 3) |> 
  step_ns(starts_with("año_construccion"), deg_free = 3) |> 
  step_unknown(calidad_sotano, calidad_garage) |> 
  step_other(nombre_zona, threshold = 0.01, other = "otras") |> 
  step_dummy(nombre_zona, calidad_garage, calidad_sotano, aire_acondicionado) |> 
  step_interact(terms = ~ starts_with("area_garage_m2_o"):starts_with("calidad_garage")) |> 
  step_interact(terms = ~ starts_with("area_sotano_m2_o"): starts_with("calidad_sotano")) |> 
  step_nzv(all_predictors(), freq_cut = 500 / 1, unique_cut = 1)
```

Usaremos regresión ridge:

```{r}
flujo_casas <- workflow() |> 
  add_recipe(receta_casas) |> 
  add_model(linear_reg(mixture = 0, penalty = 0.005) |> 
            set_engine("glmnet", lambda.min.ratio = 1e-20))  
ajuste <- fit(flujo_casas, casas_entrena)
```

Para medir el desempeño convertimos a la variable de precio multiplicando
la predicción por el área habitable:

```{r}
metricas <- metric_set(mape, mae, rmse, rsq)
casas_prueba_normal <- testing(casas_split) |> 
  filter(condicion_venta == "Normal")
metricas(casas_prueba_normal |> bind_cols(predict(ajuste, casas_prueba_normal)), 
     truth = precio_miles, estimate = .pred * area_hab_m2 ) |> 
  gt() |> fmt_number(.estimate, decimals = 2)
```


La distribución de los residuales de ve como sigue:

```{r}
preds_tbl <- casas_prueba_normal |> 
  bind_cols(predict(ajuste, casas_prueba_normal)) |> 
  mutate(residual = precio_m2_miles - .pred)
preds_tbl |> ggplot(aes(x = residual)) + geom_histogram(binwidth = 0.05)
intervalo_r <- quantile(preds_tbl$residual, c(0.05, 0.95))
```


Y nuestros intervalos se ven razonables, con menos incertidumbre para casas
más baratas y más incertidumbre para las más caras:

```{r}
set.seed(832)
preds_tbl <- preds_tbl |> 
  mutate(inf = .pred + intervalo_r[1], sup = .pred + intervalo_r[2]) |> 
  mutate(pred_precio = .pred * area_hab_m2) |> 
  mutate(inf_precio = area_hab_m2 * inf, sup_precio = area_hab_m2 * sup)
ggplot(preds_tbl |> sample_n(200), aes(x = pred_precio, y = precio_miles, ymin = inf_precio, 
                      ymax = sup_precio)) +
  geom_abline() + 
  geom_linerange() +
  geom_point(colour = "red") + coord_obs_pred()
```

Checamos finalmente que la calibración es razonable con una tabla:

```{r}
n_grupos <- 8 # podemos usar más grupos si tenemos más datos
preds_tbl |> 
  rename(y = precio_miles) |> 
  mutate(grupo_pred = cut_number(pred_precio, n = n_grupos)) |> 
  group_by(grupo_pred) |> 
  summarise(n = n(), cobertura = mean(y >= inf_precio & y <= sup_precio), 
            pred_precio = mean(pred_precio)) |>
  mutate(error_e = 2 * sqrt(cobertura * (1- cobertura) / n)) |> 
  ggplot(aes(x = pred_precio, y = cobertura, ymin = cobertura - error_e, ymax = cobertura + error_e)) +
  geom_hline(yintercept = 0.9, colour = "red") +
  geom_point() + geom_linerange() + ylim(c(0.5, 1.1))
```
